{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Module.ModelModule import CustomDataset\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchsummary import summary\n",
    "from torchmetrics.regression import R2Score, MeanAbsoluteError, MeanAbsolutePercentageError, MeanSquaredError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../Data/'\n",
    "\n",
    "electric_df = pd.read_csv(DATA_PATH + 'electric_df.csv', index_col = 0)\n",
    "water_df = pd.read_csv(DATA_PATH + 'water_df.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0     1     2     3     4     5     6     7     8     9  ...    19  \\\n",
      "0  1.23  1.45  1.73  1.62  1.31  1.48  1.74  1.71  1.30  1.49  ...  1.68   \n",
      "1  1.45  1.73  1.62  1.31  1.48  1.74  1.71  1.30  1.49  1.94  ...  1.20   \n",
      "2  1.73  1.62  1.31  1.48  1.74  1.71  1.30  1.49  1.94  1.68  ...  1.42   \n",
      "3  1.62  1.31  1.48  1.74  1.71  1.30  1.49  1.94  1.68  1.26  ...  1.88   \n",
      "4  1.31  1.48  1.74  1.71  1.30  1.49  1.94  1.68  1.26  1.39  ...  1.67   \n",
      "\n",
      "     20    21    22    23    24    25    26    27    28  \n",
      "0  1.20  1.42  1.88  1.67  1.24  1.39  1.67  1.60  1.26  \n",
      "1  1.42  1.88  1.67  1.24  1.39  1.67  1.60  1.26  1.41  \n",
      "2  1.88  1.67  1.24  1.39  1.67  1.60  1.26  1.41  1.68  \n",
      "3  1.67  1.24  1.39  1.67  1.60  1.26  1.41  1.68  1.59  \n",
      "4  1.24  1.39  1.67  1.60  1.26  1.41  1.68  1.59  1.24  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "\n",
      "     0    1    2    3    4    5    6    7    8    9  ...   19   20   21   22  \\\n",
      "0   30  120  210  410   32  184  180  260   35  145  ...   95   46  139  204   \n",
      "1  120  210  410   32  184  180  260   35  145  203  ...   46  139  204  198   \n",
      "2  210  410   32  184  180  260   35  145  203  216  ...  139  204  198   53   \n",
      "3  410   32  184  180  260   35  145  203  216   43  ...  204  198   53  162   \n",
      "4   32  184  180  260   35  145  203  216   43  136  ...  198   53  162  210   \n",
      "\n",
      "    23   24   25   26   27   28  \n",
      "0  198   53  162  210  150   51  \n",
      "1   53  162  210  150   51  169  \n",
      "2  162  210  150   51  169  204  \n",
      "3  210  150   51  169  204  169  \n",
      "4  150   51  169  204  169   38  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "print(electric_df.head())\n",
    "print()\n",
    "print(water_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "electric_features = electric_df[electric_df.columns[:-1]]\n",
    "electric_target = electric_df[electric_df.columns[-1:]]\n",
    "\n",
    "electric_X_train, electric_X_test, electric_y_train, electric_y_test = train_test_split(electric_features,\n",
    "                                                    electric_target,\n",
    "                                                    random_state = 42,\n",
    "                                                    test_size = 0.2)\n",
    "\n",
    "water_features = water_df[water_df.columns[:-1]]\n",
    "water_target = water_df[water_df.columns[-1:]]\n",
    "\n",
    "water_X_train, water_X_test, water_y_train, water_y_test = train_test_split(water_features,\n",
    "                                                                            water_target,\n",
    "                                                                            random_state = 42,\n",
    "                                                                            test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "electric_rbscaler = RobustScaler().fit(electric_X_train)\n",
    "water_rbscaler = RobustScaler().fit(water_X_train)\n",
    "\n",
    "with open('electric_min_max_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(electric_rbscaler, f)\n",
    "\n",
    "with open('water_robust_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(water_rbscaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "electric_X_train_scaled = electric_rbscaler.transform(electric_X_train)\n",
    "electric_X_test_scaled = electric_rbscaler.transform(electric_X_test)\n",
    "\n",
    "water_X_train_scaled = water_rbscaler.transform(water_X_train)\n",
    "water_X_test_scaled = water_rbscaler.transform(water_X_test)\n",
    "\n",
    "electric_X_train = pd.DataFrame(electric_X_train_scaled, columns = electric_X_train.columns)\n",
    "electric_X_test = pd.DataFrame(electric_X_test_scaled, columns = electric_X_test.columns)\n",
    "\n",
    "water_X_train = pd.DataFrame(water_X_train_scaled, columns = water_X_train.columns)\n",
    "water_X_test = pd.DataFrame(water_X_test_scaled, columns = water_X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100000\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "electric_trainDS = CustomDataset(electric_X_train, electric_y_train)\n",
    "water_trainDS = CustomDataset(water_X_train, water_y_train)\n",
    "\n",
    "electric_trainDL = DataLoader(electric_trainDS, batch_size = BATCH_SIZE)\n",
    "water_trainDL = DataLoader(water_trainDS, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, latent_dim, n_layers, dropout,\n",
    "                 bidirectional):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.GRU(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_dim,\n",
    "            num_layers = n_layers,\n",
    "            dropout = dropout,\n",
    "            bidirectional = bidirectional,\n",
    "            batch_first = True\n",
    "        )\n",
    "\n",
    "        self.mu = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, latent_dim)\n",
    "        self.logvar = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, latent_dim)\n",
    "\n",
    "        self.decoder = nn.GRU(\n",
    "            input_size = latent_dim,\n",
    "            hidden_size = hidden_dim,\n",
    "            num_layers = n_layers,\n",
    "            dropout = dropout,\n",
    "            bidirectional = bidirectional,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.output = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, 1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        encoder, hidden = self.encoder(inputs)\n",
    "        \n",
    "        if self.encoder.bidirectional:\n",
    "            hidden = torch.cat([hidden[-2], hidden[-1]], dim=-1)\n",
    "        else:\n",
    "            hidden = hidden[-1]\n",
    "\n",
    "        mu = self.mu(hidden)\n",
    "        logvar = self.logvar(hidden)\n",
    "        \n",
    "        reparameter = self.reparameterize(mu, logvar)\n",
    "        reparameter = reparameter.unsqueeze(1)\n",
    "\n",
    "        decoder, _ = self.decoder(reparameter)\n",
    "\n",
    "        reconstruction = self.output(decoder)\n",
    "\n",
    "        return reconstruction, mu, logvar\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAE_loss(reconstruction, target, mu, logvar):\n",
    "    MAE_loss = nn.L1Loss(reduction = 'mean')\n",
    "    loss_val = MAE_loss(reconstruction, target)\n",
    "\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return loss_val + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(featureDF, targetDF, model, DEVICE):\n",
    "    featureTS = torch.FloatTensor(featureDF.values).unsqueeze(1).to(DEVICE)\n",
    "    targetTS = torch.FloatTensor(targetDF.values).unsqueeze(1).to(DEVICE)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        reconstruction, mu, logvar = model(featureTS)\n",
    "        vae_loss_val = VAE_loss(reconstruction, targetTS, mu, logvar)\n",
    "\n",
    "    return vae_loss_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(testDF, testtargetDF, model, trainDL,\n",
    "              optimizer, EPOCH, scheduler, DEVICE, accumulation_steps):\n",
    "    SAVE_PATH = './saved_models/'\n",
    "    os.makedirs(SAVE_PATH, exist_ok = True)\n",
    "\n",
    "    BREAK_CNT_LOSS = 0\n",
    "    BREAK_CNT_SCORE = 0\n",
    "    LIMIT_VALUE = 10\n",
    "\n",
    "    VAE_LOSS_HISTORY = [[], []]\n",
    "\n",
    "    for epoch in range(1, EPOCH + 1):\n",
    "        SAVE_MODEL = os.path.join(SAVE_PATH, f'model_{epoch}.pth')\n",
    "        SAVE_WEIGHT = os.path.join(SAVE_PATH, f'model_weights_{epoch}.pth')\n",
    "\n",
    "        vae_loss_total = 0\n",
    "\n",
    "        for step, (featureTS, targetTS) in enumerate(trainDL):\n",
    "            featureTS = featureTS.unsqueeze(1).to(DEVICE)\n",
    "            targetTS = targetTS.unsqueeze(1).to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            reconstruction, mu, logvar = model(featureTS)\n",
    "            vae_loss = VAE_loss(reconstruction, targetTS, mu, logvar)\n",
    "\n",
    "            # Loss 누적 및 Backward pass\n",
    "            vae_loss = vae_loss / accumulation_steps  # 누적 단계로 나눔\n",
    "            vae_loss.backward()\n",
    "            vae_loss_total += vae_loss.item() * accumulation_steps\n",
    "\n",
    "            # Accumulation 단계마다 가중치 업데이트\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        test_vae_loss = testing(testDF, testtargetDF, model, DEVICE)\n",
    "\n",
    "        VAE_LOSS_HISTORY[1].append(test_vae_loss)\n",
    "        VAE_LOSS_HISTORY[0].append(vae_loss_total / len(trainDL))\n",
    "\n",
    "        print(f'[{epoch} / {EPOCH}]\\n- TRAIN VAE LOSS : {VAE_LOSS_HISTORY[0][-1]}')\n",
    "        print(f'\\n- TEST VAE LOSS : {VAE_LOSS_HISTORY[1][-1]}')\n",
    "        scheduler.step(test_vae_loss)\n",
    "\n",
    "        if len(VAE_LOSS_HISTORY[1]) >= 2:\n",
    "            if VAE_LOSS_HISTORY[1][-1] >= VAE_LOSS_HISTORY[1][-2]: BREAK_CNT_LOSS += 1\n",
    "        \n",
    "        if len(VAE_LOSS_HISTORY[1]) == 1:\n",
    "            torch.save(model.state_dict(), SAVE_WEIGHT)\n",
    "            torch.save(model, SAVE_MODEL)\n",
    "\n",
    "        else:\n",
    "            if VAE_LOSS_HISTORY[1][-1] < min(VAE_LOSS_HISTORY[1][:-1]):\n",
    "                torch.save(model.state_dict(), SAVE_WEIGHT)\n",
    "                torch.save(model, SAVE_MODEL)\n",
    "\n",
    "        if BREAK_CNT_LOSS > LIMIT_VALUE:\n",
    "            print(f\"성능 및 손실 개선이 없어서 {epoch} EPOCH에 학습 중단\")\n",
    "            # break\n",
    "\n",
    "    return VAE_LOSS_HISTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KDP-2\\anaconda3\\envs\\Project_38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_size = 28\n",
    "hidden_dim = 64\n",
    "latent_dim = 32\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "bidirectional = True\n",
    "accumulation_steps = 2\n",
    "\n",
    "\n",
    "vae_model = VAEModel(input_size = input_size, hidden_dim = hidden_dim,\n",
    "                     latent_dim = latent_dim, n_layers = n_layers,\n",
    "                     dropout = dropout, bidirectional = bidirectional)\n",
    "\n",
    "optimizer = optim.Adam(vae_model.parameters(), lr = LR)\n",
    "\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', patience = 100, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 / 100000]\n",
      "- TRAIN VAE LOSS : 68.03102227985158\n",
      "\n",
      "- TEST VAE LOSS : 60.31974411010742\n",
      "[2 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23587191951976\n",
      "\n",
      "- TEST VAE LOSS : 60.29950714111328\n",
      "[3 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23522606546739\n",
      "\n",
      "- TEST VAE LOSS : 60.277217864990234\n",
      "[4 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23519761545518\n",
      "\n",
      "- TEST VAE LOSS : 60.27719497680664\n",
      "[5 / 100000]\n",
      "- TRAIN VAE LOSS : 60.236816028370576\n",
      "\n",
      "- TEST VAE LOSS : 60.27898025512695\n",
      "[6 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23571530196246\n",
      "\n",
      "- TEST VAE LOSS : 60.277366638183594\n",
      "[7 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2351417240816\n",
      "\n",
      "- TEST VAE LOSS : 60.27705764770508\n",
      "[8 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235180975072524\n",
      "\n",
      "- TEST VAE LOSS : 60.27873992919922\n",
      "[9 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23514332490809\n",
      "\n",
      "- TEST VAE LOSS : 60.27680587768555\n",
      "[10 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23519342826395\n",
      "\n",
      "- TEST VAE LOSS : 60.2785530090332\n",
      "[11 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23517090786205\n",
      "\n",
      "- TEST VAE LOSS : 60.27680206298828\n",
      "[12 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23875995389153\n",
      "\n",
      "- TEST VAE LOSS : 60.29899215698242\n",
      "[13 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235674508711874\n",
      "\n",
      "- TEST VAE LOSS : 60.27656173706055\n",
      "[14 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2350918080947\n",
      "\n",
      "- TEST VAE LOSS : 60.27621841430664\n",
      "[15 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2352085795683\n",
      "\n",
      "- TEST VAE LOSS : 60.27741241455078\n",
      "[16 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23606992654239\n",
      "\n",
      "- TEST VAE LOSS : 60.276004791259766\n",
      "[17 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235211355321546\n",
      "\n",
      "- TEST VAE LOSS : 60.27709197998047\n",
      "[18 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235199108572566\n",
      "\n",
      "- TEST VAE LOSS : 60.2771110534668\n",
      "[19 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235162694594436\n",
      "\n",
      "- TEST VAE LOSS : 60.27722930908203\n",
      "[20 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23514921121036\n",
      "\n",
      "- TEST VAE LOSS : 60.2768440246582\n",
      "[21 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23509722675997\n",
      "\n",
      "- TEST VAE LOSS : 60.27676010131836\n",
      "[22 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23523343389174\n",
      "\n",
      "- TEST VAE LOSS : 60.2763671875\n",
      "[23 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23520539137896\n",
      "\n",
      "- TEST VAE LOSS : 60.286258697509766\n",
      "[24 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23513561652688\n",
      "\n",
      "- TEST VAE LOSS : 60.27629470825195\n",
      "[25 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23515582522224\n",
      "\n",
      "- TEST VAE LOSS : 60.277557373046875\n",
      "[26 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235157531289495\n",
      "\n",
      "- TEST VAE LOSS : 60.27619934082031\n",
      "[27 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23567896113676\n",
      "\n",
      "- TEST VAE LOSS : 60.27794647216797\n",
      "성능 및 손실 개선이 없어서 27 EPOCH에 학습 중단\n",
      "[28 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235445856655346\n",
      "\n",
      "- TEST VAE LOSS : 60.278099060058594\n",
      "성능 및 손실 개선이 없어서 28 EPOCH에 학습 중단\n",
      "[29 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23521205408433\n",
      "\n",
      "- TEST VAE LOSS : 60.27678680419922\n",
      "성능 및 손실 개선이 없어서 29 EPOCH에 학습 중단\n",
      "[30 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235184018303364\n",
      "\n",
      "- TEST VAE LOSS : 60.27683639526367\n",
      "성능 및 손실 개선이 없어서 30 EPOCH에 학습 중단\n",
      "[31 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23519945436365\n",
      "\n",
      "- TEST VAE LOSS : 60.27773666381836\n",
      "성능 및 손실 개선이 없어서 31 EPOCH에 학습 중단\n",
      "[32 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2354420004452\n",
      "\n",
      "- TEST VAE LOSS : 60.40095138549805\n",
      "성능 및 손실 개선이 없어서 32 EPOCH에 학습 중단\n",
      "[33 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235997751572555\n",
      "\n",
      "- TEST VAE LOSS : 60.27564239501953\n",
      "성능 및 손실 개선이 없어서 33 EPOCH에 학습 중단\n",
      "[34 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235514714633716\n",
      "\n",
      "- TEST VAE LOSS : 60.27574157714844\n",
      "성능 및 손실 개선이 없어서 34 EPOCH에 학습 중단\n",
      "[35 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235350684446445\n",
      "\n",
      "- TEST VAE LOSS : 60.27912521362305\n",
      "성능 및 손실 개선이 없어서 35 EPOCH에 학습 중단\n",
      "[36 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23551783774881\n",
      "\n",
      "- TEST VAE LOSS : 60.2763557434082\n",
      "성능 및 손실 개선이 없어서 36 EPOCH에 학습 중단\n",
      "[37 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2351688414181\n",
      "\n",
      "- TEST VAE LOSS : 60.276615142822266\n",
      "성능 및 손실 개선이 없어서 37 EPOCH에 학습 중단\n",
      "[38 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235181286531336\n",
      "\n",
      "- TEST VAE LOSS : 60.27751159667969\n",
      "성능 및 손실 개선이 없어서 38 EPOCH에 학습 중단\n",
      "[39 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23518305789723\n",
      "\n",
      "- TEST VAE LOSS : 60.27651596069336\n",
      "성능 및 손실 개선이 없어서 39 EPOCH에 학습 중단\n",
      "[40 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23546733025943\n",
      "\n",
      "- TEST VAE LOSS : 60.37017822265625\n",
      "성능 및 손실 개선이 없어서 40 EPOCH에 학습 중단\n",
      "[41 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235342477013084\n",
      "\n",
      "- TEST VAE LOSS : 60.280113220214844\n",
      "성능 및 손실 개선이 없어서 41 EPOCH에 학습 중단\n",
      "[42 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23525348147224\n",
      "\n",
      "- TEST VAE LOSS : 60.276817321777344\n",
      "성능 및 손실 개선이 없어서 42 EPOCH에 학습 중단\n",
      "[43 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235193144181196\n",
      "\n",
      "- TEST VAE LOSS : 60.27680587768555\n",
      "성능 및 손실 개선이 없어서 43 EPOCH에 학습 중단\n",
      "[44 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23518836548749\n",
      "\n",
      "- TEST VAE LOSS : 60.33181381225586\n",
      "성능 및 손실 개선이 없어서 44 EPOCH에 학습 중단\n",
      "[45 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23516914973539\n",
      "\n",
      "- TEST VAE LOSS : 60.332191467285156\n",
      "성능 및 손실 개선이 없어서 45 EPOCH에 학습 중단\n",
      "[46 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23526959767061\n",
      "\n",
      "- TEST VAE LOSS : 60.280513763427734\n",
      "성능 및 손실 개선이 없어서 46 EPOCH에 학습 중단\n",
      "[47 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23528957568898\n",
      "\n",
      "- TEST VAE LOSS : 60.275997161865234\n",
      "성능 및 손실 개선이 없어서 47 EPOCH에 학습 중단\n",
      "[48 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23514627793256\n",
      "\n",
      "- TEST VAE LOSS : 60.27610397338867\n",
      "성능 및 손실 개선이 없어서 48 EPOCH에 학습 중단\n",
      "[49 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2350773301966\n",
      "\n",
      "- TEST VAE LOSS : 60.27736282348633\n",
      "성능 및 손실 개선이 없어서 49 EPOCH에 학습 중단\n",
      "[50 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23519391362807\n",
      "\n",
      "- TEST VAE LOSS : 60.27725601196289\n",
      "성능 및 손실 개선이 없어서 50 EPOCH에 학습 중단\n",
      "[51 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23515461551442\n",
      "\n",
      "- TEST VAE LOSS : 60.27687454223633\n",
      "성능 및 손실 개선이 없어서 51 EPOCH에 학습 중단\n",
      "[52 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235102220871866\n",
      "\n",
      "- TEST VAE LOSS : 60.27729797363281\n",
      "성능 및 손실 개선이 없어서 52 EPOCH에 학습 중단\n",
      "[53 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23521532440186\n",
      "\n",
      "- TEST VAE LOSS : 60.27704620361328\n",
      "성능 및 손실 개선이 없어서 53 EPOCH에 학습 중단\n",
      "[54 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235173361834356\n",
      "\n",
      "- TEST VAE LOSS : 60.27669906616211\n",
      "성능 및 손실 개선이 없어서 54 EPOCH에 학습 중단\n",
      "[55 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23523361654843\n",
      "\n",
      "- TEST VAE LOSS : 60.276248931884766\n",
      "성능 및 손실 개선이 없어서 55 EPOCH에 학습 중단\n",
      "[56 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23520442379222\n",
      "\n",
      "- TEST VAE LOSS : 60.276893615722656\n",
      "성능 및 손실 개선이 없어서 56 EPOCH에 학습 중단\n",
      "[57 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23517457176657\n",
      "\n",
      "- TEST VAE LOSS : 60.27677917480469\n",
      "성능 및 손실 개선이 없어서 57 EPOCH에 학습 중단\n",
      "[58 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23514287971048\n",
      "\n",
      "- TEST VAE LOSS : 60.276084899902344\n",
      "성능 및 손실 개선이 없어서 58 EPOCH에 학습 중단\n",
      "[59 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23516903103099\n",
      "\n",
      "- TEST VAE LOSS : 60.2774658203125\n",
      "성능 및 손실 개선이 없어서 59 EPOCH에 학습 중단\n",
      "[60 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23517178613999\n",
      "\n",
      "- TEST VAE LOSS : 60.27682113647461\n",
      "성능 및 손실 개선이 없어서 60 EPOCH에 학습 중단\n",
      "[61 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23520346832275\n",
      "\n",
      "- TEST VAE LOSS : 60.27714538574219\n",
      "성능 및 손실 개선이 없어서 61 EPOCH에 학습 중단\n",
      "[62 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2352051559897\n",
      "\n",
      "- TEST VAE LOSS : 60.32435607910156\n",
      "성능 및 손실 개선이 없어서 62 EPOCH에 학습 중단\n",
      "[63 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235360411924475\n",
      "\n",
      "- TEST VAE LOSS : 60.35456466674805\n",
      "성능 및 손실 개선이 없어서 63 EPOCH에 학습 중단\n",
      "[64 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23542369483499\n",
      "\n",
      "- TEST VAE LOSS : 60.30364990234375\n",
      "성능 및 손실 개선이 없어서 64 EPOCH에 학습 중단\n",
      "[65 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23523953493903\n",
      "\n",
      "- TEST VAE LOSS : 60.27610397338867\n",
      "성능 및 손실 개선이 없어서 65 EPOCH에 학습 중단\n",
      "[66 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235137451620666\n",
      "\n",
      "- TEST VAE LOSS : 60.27608871459961\n",
      "성능 및 손실 개선이 없어서 66 EPOCH에 학습 중단\n",
      "[67 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235177177877986\n",
      "\n",
      "- TEST VAE LOSS : 60.27686309814453\n",
      "성능 및 손실 개선이 없어서 67 EPOCH에 학습 중단\n",
      "[68 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2351401124842\n",
      "\n",
      "- TEST VAE LOSS : 60.27702331542969\n",
      "성능 및 손실 개선이 없어서 68 EPOCH에 학습 중단\n",
      "[69 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235168897740984\n",
      "\n",
      "- TEST VAE LOSS : 60.27725601196289\n",
      "성능 및 손실 개선이 없어서 69 EPOCH에 학습 중단\n",
      "[70 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23514925249885\n",
      "\n",
      "- TEST VAE LOSS : 60.27790069580078\n",
      "성능 및 손실 개선이 없어서 70 EPOCH에 학습 중단\n",
      "[71 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23509722810633\n",
      "\n",
      "- TEST VAE LOSS : 60.276920318603516\n",
      "성능 및 손실 개선이 없어서 71 EPOCH에 학습 중단\n",
      "[72 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23521376755658\n",
      "\n",
      "- TEST VAE LOSS : 60.2773551940918\n",
      "성능 및 손실 개선이 없어서 72 EPOCH에 학습 중단\n",
      "[73 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23517971756879\n",
      "\n",
      "- TEST VAE LOSS : 60.27670669555664\n",
      "성능 및 손실 개선이 없어서 73 EPOCH에 학습 중단\n",
      "[74 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23509118607465\n",
      "\n",
      "- TEST VAE LOSS : 60.276695251464844\n",
      "성능 및 손실 개선이 없어서 74 EPOCH에 학습 중단\n",
      "[75 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235204753875735\n",
      "\n",
      "- TEST VAE LOSS : 60.27733612060547\n",
      "성능 및 손실 개선이 없어서 75 EPOCH에 학습 중단\n",
      "[76 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2351796502506\n",
      "\n",
      "- TEST VAE LOSS : 60.2773551940918\n",
      "성능 및 손실 개선이 없어서 76 EPOCH에 학습 중단\n",
      "[77 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23518270335478\n",
      "\n",
      "- TEST VAE LOSS : 60.276756286621094\n",
      "성능 및 손실 개선이 없어서 77 EPOCH에 학습 중단\n",
      "[78 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23519953738942\n",
      "\n",
      "- TEST VAE LOSS : 60.276981353759766\n",
      "성능 및 손실 개선이 없어서 78 EPOCH에 학습 중단\n",
      "[79 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235179643518784\n",
      "\n",
      "- TEST VAE LOSS : 60.27679443359375\n",
      "성능 및 손실 개선이 없어서 79 EPOCH에 학습 중단\n",
      "[80 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23509598810533\n",
      "\n",
      "- TEST VAE LOSS : 60.27660369873047\n",
      "성능 및 손실 개선이 없어서 80 EPOCH에 학습 중단\n",
      "[81 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23520733911851\n",
      "\n",
      "- TEST VAE LOSS : 60.27760314941406\n",
      "성능 및 손실 개선이 없어서 81 EPOCH에 학습 중단\n",
      "[82 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235179699168484\n",
      "\n",
      "- TEST VAE LOSS : 60.277469635009766\n",
      "성능 및 손실 개선이 없어서 82 EPOCH에 학습 중단\n",
      "[83 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23518269213508\n",
      "\n",
      "- TEST VAE LOSS : 60.27698516845703\n",
      "성능 및 손실 개선이 없어서 83 EPOCH에 학습 중단\n",
      "[84 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23518723566392\n",
      "\n",
      "- TEST VAE LOSS : 60.2763671875\n",
      "성능 및 손실 개선이 없어서 84 EPOCH에 학습 중단\n",
      "[85 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23513390664493\n",
      "\n",
      "- TEST VAE LOSS : 60.27686309814453\n",
      "성능 및 손실 개선이 없어서 85 EPOCH에 학습 중단\n",
      "[86 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23509335596421\n",
      "\n",
      "- TEST VAE LOSS : 60.277130126953125\n",
      "성능 및 손실 개선이 없어서 86 EPOCH에 학습 중단\n",
      "[87 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23515173070571\n",
      "\n",
      "- TEST VAE LOSS : 60.277496337890625\n",
      "성능 및 손실 개선이 없어서 87 EPOCH에 학습 중단\n",
      "[88 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23518905145982\n",
      "\n",
      "- TEST VAE LOSS : 60.276702880859375\n",
      "성능 및 손실 개선이 없어서 88 EPOCH에 학습 중단\n",
      "[89 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23518128967285\n",
      "\n",
      "- TEST VAE LOSS : 60.27732849121094\n",
      "성능 및 손실 개선이 없어서 89 EPOCH에 학습 중단\n",
      "[90 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23517987554214\n",
      "\n",
      "- TEST VAE LOSS : 60.27735137939453\n",
      "성능 및 손실 개선이 없어서 90 EPOCH에 학습 중단\n",
      "[91 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23519941801183\n",
      "\n",
      "- TEST VAE LOSS : 60.27675247192383\n",
      "성능 및 손실 개선이 없어서 91 EPOCH에 학습 중단\n",
      "[92 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235162638944736\n",
      "\n",
      "- TEST VAE LOSS : 60.27693557739258\n",
      "성능 및 손실 개선이 없어서 92 EPOCH에 학습 중단\n",
      "[93 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23517970455394\n",
      "\n",
      "- TEST VAE LOSS : 60.276851654052734\n",
      "성능 및 손실 개선이 없어서 93 EPOCH에 학습 중단\n",
      "[94 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23509117934283\n",
      "\n",
      "- TEST VAE LOSS : 60.27635192871094\n",
      "성능 및 손실 개선이 없어서 94 EPOCH에 학습 중단\n",
      "[95 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23520485373104\n",
      "\n",
      "- TEST VAE LOSS : 60.27716064453125\n",
      "성능 및 손실 개선이 없어서 95 EPOCH에 학습 중단\n",
      "[96 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23517967313879\n",
      "\n",
      "- TEST VAE LOSS : 60.277217864990234\n",
      "성능 및 손실 개선이 없어서 96 EPOCH에 학습 중단\n",
      "[97 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23518264052447\n",
      "\n",
      "- TEST VAE LOSS : 60.27633285522461\n",
      "성능 및 손실 개선이 없어서 97 EPOCH에 학습 중단\n",
      "[98 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23519945930032\n",
      "\n",
      "- TEST VAE LOSS : 60.276607513427734\n",
      "성능 및 손실 개선이 없어서 98 EPOCH에 학습 중단\n",
      "[99 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235149084203385\n",
      "\n",
      "- TEST VAE LOSS : 60.27742004394531\n",
      "성능 및 손실 개선이 없어서 99 EPOCH에 학습 중단\n",
      "[100 / 100000]\n",
      "- TRAIN VAE LOSS : 60.235097298117246\n",
      "\n",
      "- TEST VAE LOSS : 60.276546478271484\n",
      "성능 및 손실 개선이 없어서 100 EPOCH에 학습 중단\n",
      "[101 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23521377720552\n",
      "\n",
      "- TEST VAE LOSS : 60.277061462402344\n",
      "성능 및 손실 개선이 없어서 101 EPOCH에 학습 중단\n",
      "[102 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23517974135455\n",
      "\n",
      "- TEST VAE LOSS : 60.27695846557617\n",
      "성능 및 손실 개선이 없어서 102 EPOCH에 학습 중단\n",
      "[103 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2351014027315\n",
      "\n",
      "- TEST VAE LOSS : 60.27627182006836\n",
      "성능 및 손실 개선이 없어서 103 EPOCH에 학습 중단\n",
      "[104 / 100000]\n",
      "- TRAIN VAE LOSS : 60.23520727718578\n",
      "\n",
      "- TEST VAE LOSS : 60.277252197265625\n",
      "성능 및 손실 개선이 없어서 104 EPOCH에 학습 중단\n",
      "[105 / 100000]\n",
      "- TRAIN VAE LOSS : 60.229189266878016\n",
      "\n",
      "- TEST VAE LOSS : 60.26420974731445\n",
      "성능 및 손실 개선이 없어서 105 EPOCH에 학습 중단\n",
      "[106 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756744160372\n",
      "\n",
      "- TEST VAE LOSS : 60.263797760009766\n",
      "성능 및 손실 개선이 없어서 106 EPOCH에 학습 중단\n",
      "[107 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757105703915\n",
      "\n",
      "- TEST VAE LOSS : 60.26388168334961\n",
      "성능 및 손실 개선이 없어서 107 EPOCH에 학습 중단\n",
      "[108 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2275745230282\n",
      "\n",
      "- TEST VAE LOSS : 60.264366149902344\n",
      "성능 및 손실 개선이 없어서 108 EPOCH에 학습 중단\n",
      "[109 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757612609863\n",
      "\n",
      "- TEST VAE LOSS : 60.26411056518555\n",
      "성능 및 손실 개선이 없어서 109 EPOCH에 학습 중단\n",
      "[110 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757012400908\n",
      "\n",
      "- TEST VAE LOSS : 60.26450729370117\n",
      "성능 및 손실 개선이 없어서 110 EPOCH에 학습 중단\n",
      "[111 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227563308491426\n",
      "\n",
      "- TEST VAE LOSS : 60.264156341552734\n",
      "성능 및 손실 개선이 없어서 111 EPOCH에 학습 중단\n",
      "[112 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227571099674\n",
      "\n",
      "- TEST VAE LOSS : 60.264404296875\n",
      "성능 및 손실 개선이 없어서 112 EPOCH에 학습 중단\n",
      "[113 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757473620246\n",
      "\n",
      "- TEST VAE LOSS : 60.264060974121094\n",
      "성능 및 손실 개선이 없어서 113 EPOCH에 학습 중단\n",
      "[114 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757471443625\n",
      "\n",
      "- TEST VAE LOSS : 60.26384735107422\n",
      "성능 및 손실 개선이 없어서 114 EPOCH에 학습 중단\n",
      "[115 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757499605066\n",
      "\n",
      "- TEST VAE LOSS : 60.26493835449219\n",
      "성능 및 손실 개선이 없어서 115 EPOCH에 학습 중단\n",
      "[116 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756794469497\n",
      "\n",
      "- TEST VAE LOSS : 60.26393127441406\n",
      "성능 및 손실 개선이 없어서 116 EPOCH에 학습 중단\n",
      "[117 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2275734091366\n",
      "\n",
      "- TEST VAE LOSS : 60.26411056518555\n",
      "성능 및 손실 개선이 없어서 117 EPOCH에 학습 중단\n",
      "[118 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757227863985\n",
      "\n",
      "- TEST VAE LOSS : 60.264060974121094\n",
      "성능 및 손실 개선이 없어서 118 EPOCH에 학습 중단\n",
      "[119 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227576221690455\n",
      "\n",
      "- TEST VAE LOSS : 60.26374053955078\n",
      "성능 및 손실 개선이 없어서 119 EPOCH에 학습 중단\n",
      "[120 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756986460966\n",
      "\n",
      "- TEST VAE LOSS : 60.263893127441406\n",
      "성능 및 손실 개선이 없어서 120 EPOCH에 학습 중단\n",
      "[121 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756826468075\n",
      "\n",
      "- TEST VAE LOSS : 60.26437759399414\n",
      "성능 및 손실 개선이 없어서 121 EPOCH에 학습 중단\n",
      "[122 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757262106503\n",
      "\n",
      "- TEST VAE LOSS : 60.264373779296875\n",
      "성능 및 손실 개선이 없어서 122 EPOCH에 학습 중단\n",
      "[123 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227571386449476\n",
      "\n",
      "- TEST VAE LOSS : 60.26417541503906\n",
      "성능 및 손실 개선이 없어서 123 EPOCH에 학습 중단\n",
      "[124 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2275723324944\n",
      "\n",
      "- TEST VAE LOSS : 60.26401138305664\n",
      "성능 및 손실 개선이 없어서 124 EPOCH에 학습 중단\n",
      "[125 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757609333711\n",
      "\n",
      "- TEST VAE LOSS : 60.26383590698242\n",
      "성능 및 손실 개선이 없어서 125 EPOCH에 학습 중단\n",
      "[126 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227569892434516\n",
      "\n",
      "- TEST VAE LOSS : 60.26422119140625\n",
      "성능 및 손실 개선이 없어서 126 EPOCH에 학습 중단\n",
      "[127 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757311921961\n",
      "\n",
      "- TEST VAE LOSS : 60.26419448852539\n",
      "성능 및 손실 개선이 없어서 127 EPOCH에 학습 중단\n",
      "[128 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757114387961\n",
      "\n",
      "- TEST VAE LOSS : 60.264102935791016\n",
      "성능 및 손실 개선이 없어서 128 EPOCH에 학습 중단\n",
      "[129 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757372755163\n",
      "\n",
      "- TEST VAE LOSS : 60.264530181884766\n",
      "성능 및 손실 개선이 없어서 129 EPOCH에 학습 중단\n",
      "[130 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227571716308596\n",
      "\n",
      "- TEST VAE LOSS : 60.2644157409668\n",
      "성능 및 손실 개선이 없어서 130 EPOCH에 학습 중단\n",
      "[131 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756646055334\n",
      "\n",
      "- TEST VAE LOSS : 60.264366149902344\n",
      "성능 및 손실 개선이 없어서 131 EPOCH에 학습 중단\n",
      "[132 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227562896728514\n",
      "\n",
      "- TEST VAE LOSS : 60.26473617553711\n",
      "성능 및 손실 개선이 없어서 132 EPOCH에 학습 중단\n",
      "[133 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227572762657616\n",
      "\n",
      "- TEST VAE LOSS : 60.264122009277344\n",
      "성능 및 손실 개선이 없어서 133 EPOCH에 학습 중단\n",
      "[134 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227579776090735\n",
      "\n",
      "- TEST VAE LOSS : 60.26430892944336\n",
      "성능 및 손실 개선이 없어서 134 EPOCH에 학습 중단\n",
      "[135 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756954641903\n",
      "\n",
      "- TEST VAE LOSS : 60.264095306396484\n",
      "성능 및 손실 개선이 없어서 135 EPOCH에 학습 중단\n",
      "[136 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757230556712\n",
      "\n",
      "- TEST VAE LOSS : 60.26438522338867\n",
      "성능 및 손실 개선이 없어서 136 EPOCH에 학습 중단\n",
      "[137 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757395553589\n",
      "\n",
      "- TEST VAE LOSS : 60.263797760009766\n",
      "성능 및 손실 개선이 없어서 137 EPOCH에 학습 중단\n",
      "[138 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756288281609\n",
      "\n",
      "- TEST VAE LOSS : 60.263916015625\n",
      "성능 및 손실 개선이 없어서 138 EPOCH에 학습 중단\n",
      "[139 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757438300638\n",
      "\n",
      "- TEST VAE LOSS : 60.263771057128906\n",
      "성능 및 손실 개선이 없어서 139 EPOCH에 학습 중단\n",
      "[140 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227571848701025\n",
      "\n",
      "- TEST VAE LOSS : 60.26436233520508\n",
      "성능 및 손실 개선이 없어서 140 EPOCH에 학습 중단\n",
      "[141 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756536595961\n",
      "\n",
      "- TEST VAE LOSS : 60.2643928527832\n",
      "성능 및 손실 개선이 없어서 141 EPOCH에 학습 중단\n",
      "[142 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756770728616\n",
      "\n",
      "- TEST VAE LOSS : 60.2641716003418\n",
      "성능 및 손실 개선이 없어서 142 EPOCH에 학습 중단\n",
      "[143 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757630696016\n",
      "\n",
      "- TEST VAE LOSS : 60.264198303222656\n",
      "성능 및 손실 개선이 없어서 143 EPOCH에 학습 중단\n",
      "[144 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227566364512725\n",
      "\n",
      "- TEST VAE LOSS : 60.26446533203125\n",
      "성능 및 손실 개선이 없어서 144 EPOCH에 학습 중단\n",
      "[145 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756629629696\n",
      "\n",
      "- TEST VAE LOSS : 60.26388168334961\n",
      "성능 및 손실 개선이 없어서 145 EPOCH에 학습 중단\n",
      "[146 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757686031566\n",
      "\n",
      "- TEST VAE LOSS : 60.263999938964844\n",
      "성능 및 손실 개선이 없어서 146 EPOCH에 학습 중단\n",
      "[147 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756993327421\n",
      "\n",
      "- TEST VAE LOSS : 60.26435089111328\n",
      "성능 및 손실 개선이 없어서 147 EPOCH에 학습 중단\n",
      "[148 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756463690365\n",
      "\n",
      "- TEST VAE LOSS : 60.26413345336914\n",
      "성능 및 손실 개선이 없어서 148 EPOCH에 학습 중단\n",
      "[149 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227572027318615\n",
      "\n",
      "- TEST VAE LOSS : 60.26384353637695\n",
      "성능 및 손실 개선이 없어서 149 EPOCH에 학습 중단\n",
      "[150 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757435338637\n",
      "\n",
      "- TEST VAE LOSS : 60.26432418823242\n",
      "성능 및 손실 개선이 없어서 150 EPOCH에 학습 중단\n",
      "[151 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227569961996636\n",
      "\n",
      "- TEST VAE LOSS : 60.264617919921875\n",
      "성능 및 손실 개선이 없어서 151 EPOCH에 학습 중단\n",
      "[152 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757980974983\n",
      "\n",
      "- TEST VAE LOSS : 60.26435470581055\n",
      "성능 및 손실 개선이 없어서 152 EPOCH에 학습 중단\n",
      "[153 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757227235682\n",
      "\n",
      "- TEST VAE LOSS : 60.26343536376953\n",
      "성능 및 손실 개선이 없어서 153 EPOCH에 학습 중단\n",
      "[154 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756776338465\n",
      "\n",
      "- TEST VAE LOSS : 60.26403045654297\n",
      "성능 및 손실 개선이 없어서 154 EPOCH에 학습 중단\n",
      "[155 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227577767316035\n",
      "\n",
      "- TEST VAE LOSS : 60.264339447021484\n",
      "성능 및 손실 개선이 없어서 155 EPOCH에 학습 중단\n",
      "[156 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756698743035\n",
      "\n",
      "- TEST VAE LOSS : 60.26399612426758\n",
      "성능 및 손실 개선이 없어서 156 EPOCH에 학습 중단\n",
      "[157 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757588106043\n",
      "\n",
      "- TEST VAE LOSS : 60.26372528076172\n",
      "성능 및 손실 개선이 없어서 157 EPOCH에 학습 중단\n",
      "[158 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756989153694\n",
      "\n",
      "- TEST VAE LOSS : 60.26413345336914\n",
      "성능 및 손실 개선이 없어서 158 EPOCH에 학습 중단\n",
      "[159 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756295641731\n",
      "\n",
      "- TEST VAE LOSS : 60.264320373535156\n",
      "성능 및 손실 개선이 없어서 159 EPOCH에 학습 중단\n",
      "[160 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757612430348\n",
      "\n",
      "- TEST VAE LOSS : 60.264122009277344\n",
      "성능 및 손실 개선이 없어서 160 EPOCH에 학습 중단\n",
      "[161 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2275627414479\n",
      "\n",
      "- TEST VAE LOSS : 60.26401901245117\n",
      "성능 및 손실 개선이 없어서 161 EPOCH에 학습 중단\n",
      "[162 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227578094482425\n",
      "\n",
      "- TEST VAE LOSS : 60.26402282714844\n",
      "성능 및 손실 개선이 없어서 162 EPOCH에 학습 중단\n",
      "[163 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227567635928885\n",
      "\n",
      "- TEST VAE LOSS : 60.26399230957031\n",
      "성능 및 손실 개선이 없어서 163 EPOCH에 학습 중단\n",
      "[164 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227574669333066\n",
      "\n",
      "- TEST VAE LOSS : 60.264408111572266\n",
      "성능 및 손실 개선이 없어서 164 EPOCH에 학습 중단\n",
      "[165 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757552651798\n",
      "\n",
      "- TEST VAE LOSS : 60.26395034790039\n",
      "성능 및 손실 개선이 없어서 165 EPOCH에 학습 중단\n",
      "[166 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227567169189456\n",
      "\n",
      "- TEST VAE LOSS : 60.264183044433594\n",
      "성능 및 손실 개선이 없어서 166 EPOCH에 학습 중단\n",
      "[167 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757147441191\n",
      "\n",
      "- TEST VAE LOSS : 60.26469039916992\n",
      "성능 및 손실 개선이 없어서 167 EPOCH에 학습 중단\n",
      "[168 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227569510740395\n",
      "\n",
      "- TEST VAE LOSS : 60.2645378112793\n",
      "성능 및 손실 개선이 없어서 168 EPOCH에 학습 중단\n",
      "[169 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227566299887265\n",
      "\n",
      "- TEST VAE LOSS : 60.26369094848633\n",
      "성능 및 손실 개선이 없어서 169 EPOCH에 학습 중단\n",
      "[170 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757158301858\n",
      "\n",
      "- TEST VAE LOSS : 60.2640495300293\n",
      "성능 및 손실 개선이 없어서 170 EPOCH에 학습 중단\n",
      "[171 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756666766896\n",
      "\n",
      "- TEST VAE LOSS : 60.264068603515625\n",
      "성능 및 손실 개선이 없어서 171 EPOCH에 학습 중단\n",
      "[172 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756560112448\n",
      "\n",
      "- TEST VAE LOSS : 60.26347351074219\n",
      "성능 및 손실 개선이 없어서 172 EPOCH에 학습 중단\n",
      "[173 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757496822582\n",
      "\n",
      "- TEST VAE LOSS : 60.2642707824707\n",
      "성능 및 손실 개선이 없어서 173 EPOCH에 학습 중단\n",
      "[174 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227572972465964\n",
      "\n",
      "- TEST VAE LOSS : 60.264244079589844\n",
      "성능 및 손실 개선이 없어서 174 EPOCH에 학습 중단\n",
      "[175 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756591841754\n",
      "\n",
      "- TEST VAE LOSS : 60.26418685913086\n",
      "성능 및 손실 개선이 없어서 175 EPOCH에 학습 중단\n",
      "[176 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757519598568\n",
      "\n",
      "- TEST VAE LOSS : 60.2643928527832\n",
      "성능 및 손실 개선이 없어서 176 EPOCH에 학습 중단\n",
      "[177 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757007553998\n",
      "\n",
      "- TEST VAE LOSS : 60.26420211791992\n",
      "성능 및 손실 개선이 없어서 177 EPOCH에 학습 중단\n",
      "[178 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756707584157\n",
      "\n",
      "- TEST VAE LOSS : 60.2640495300293\n",
      "성능 및 손실 개선이 없어서 178 EPOCH에 학습 중단\n",
      "[179 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757815506879\n",
      "\n",
      "- TEST VAE LOSS : 60.264434814453125\n",
      "성능 및 손실 개선이 없어서 179 EPOCH에 학습 중단\n",
      "[180 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2275667886173\n",
      "\n",
      "- TEST VAE LOSS : 60.26396179199219\n",
      "성능 및 손실 개선이 없어서 180 EPOCH에 학습 중단\n",
      "[181 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227571462294634\n",
      "\n",
      "- TEST VAE LOSS : 60.26383972167969\n",
      "성능 및 손실 개선이 없어서 181 EPOCH에 학습 중단\n",
      "[182 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756621012968\n",
      "\n",
      "- TEST VAE LOSS : 60.2640495300293\n",
      "성능 및 손실 개선이 없어서 182 EPOCH에 학습 중단\n",
      "[183 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757505125158\n",
      "\n",
      "- TEST VAE LOSS : 60.26415252685547\n",
      "성능 및 손실 개선이 없어서 183 EPOCH에 학습 중단\n",
      "[184 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227570356481216\n",
      "\n",
      "- TEST VAE LOSS : 60.26447677612305\n",
      "성능 및 손실 개선이 없어서 184 EPOCH에 학습 중단\n",
      "[185 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227565746531766\n",
      "\n",
      "- TEST VAE LOSS : 60.26411819458008\n",
      "성능 및 손실 개선이 없어서 185 EPOCH에 학습 중단\n",
      "[186 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757452863805\n",
      "\n",
      "- TEST VAE LOSS : 60.26428985595703\n",
      "성능 및 손실 개선이 없어서 186 EPOCH에 학습 중단\n",
      "[187 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757427260455\n",
      "\n",
      "- TEST VAE LOSS : 60.263893127441406\n",
      "성능 및 손실 개선이 없어서 187 EPOCH에 학습 중단\n",
      "[188 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757035423727\n",
      "\n",
      "- TEST VAE LOSS : 60.26418685913086\n",
      "성능 및 손실 개선이 없어서 188 EPOCH에 학습 중단\n",
      "[189 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227571114484\n",
      "\n",
      "- TEST VAE LOSS : 60.2645378112793\n",
      "성능 및 손실 개선이 없어서 189 EPOCH에 학습 중단\n",
      "[190 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756481843836\n",
      "\n",
      "- TEST VAE LOSS : 60.26416778564453\n",
      "성능 및 손실 개선이 없어서 190 EPOCH에 학습 중단\n",
      "[191 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756354881735\n",
      "\n",
      "- TEST VAE LOSS : 60.264007568359375\n",
      "성능 및 손실 개선이 없어서 191 EPOCH에 학습 중단\n",
      "[192 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757257439108\n",
      "\n",
      "- TEST VAE LOSS : 60.2640495300293\n",
      "성능 및 손실 개선이 없어서 192 EPOCH에 학습 중단\n",
      "[193 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756902851778\n",
      "\n",
      "- TEST VAE LOSS : 60.26408386230469\n",
      "성능 및 손실 개선이 없어서 193 EPOCH에 학습 중단\n",
      "[194 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757284590777\n",
      "\n",
      "- TEST VAE LOSS : 60.26399230957031\n",
      "성능 및 손실 개선이 없어서 194 EPOCH에 학습 중단\n",
      "[195 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757688701854\n",
      "\n",
      "- TEST VAE LOSS : 60.2645378112793\n",
      "성능 및 손실 개선이 없어서 195 EPOCH에 학습 중단\n",
      "[196 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756989198572\n",
      "\n",
      "- TEST VAE LOSS : 60.26433181762695\n",
      "성능 및 손실 개선이 없어서 196 EPOCH에 학습 중단\n",
      "[197 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2275731344784\n",
      "\n",
      "- TEST VAE LOSS : 60.263771057128906\n",
      "성능 및 손실 개선이 없어서 197 EPOCH에 학습 중단\n",
      "[198 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757110595703\n",
      "\n",
      "- TEST VAE LOSS : 60.26450729370117\n",
      "성능 및 손실 개선이 없어서 198 EPOCH에 학습 중단\n",
      "[199 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757375560087\n",
      "\n",
      "- TEST VAE LOSS : 60.26377487182617\n",
      "성능 및 손실 개선이 없어서 199 EPOCH에 학습 중단\n",
      "[200 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757169880587\n",
      "\n",
      "- TEST VAE LOSS : 60.26417922973633\n",
      "성능 및 손실 개선이 없어서 200 EPOCH에 학습 중단\n",
      "[201 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756644888485\n",
      "\n",
      "- TEST VAE LOSS : 60.264278411865234\n",
      "성능 및 손실 개선이 없어서 201 EPOCH에 학습 중단\n",
      "[202 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227569405948415\n",
      "\n",
      "- TEST VAE LOSS : 60.26444625854492\n",
      "성능 및 손실 개선이 없어서 202 EPOCH에 학습 중단\n",
      "[203 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22757500996309\n",
      "\n",
      "- TEST VAE LOSS : 60.264015197753906\n",
      "성능 및 손실 개선이 없어서 203 EPOCH에 학습 중단\n",
      "[204 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756589283663\n",
      "\n",
      "- TEST VAE LOSS : 60.263336181640625\n",
      "성능 및 손실 개선이 없어서 204 EPOCH에 학습 중단\n",
      "[205 / 100000]\n",
      "- TRAIN VAE LOSS : 60.227567020640656\n",
      "\n",
      "- TEST VAE LOSS : 60.26422882080078\n",
      "성능 및 손실 개선이 없어서 205 EPOCH에 학습 중단\n",
      "[206 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22756713732551\n",
      "\n",
      "- TEST VAE LOSS : 60.26376724243164\n",
      "성능 및 손실 개선이 없어서 206 EPOCH에 학습 중단\n",
      "[207 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22696026970358\n",
      "\n",
      "- TEST VAE LOSS : 60.26308059692383\n",
      "성능 및 손실 개선이 없어서 207 EPOCH에 학습 중단\n",
      "[208 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812944300036\n",
      "\n",
      "- TEST VAE LOSS : 60.263328552246094\n",
      "성능 및 손실 개선이 없어서 208 EPOCH에 학습 중단\n",
      "[209 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681180392995\n",
      "\n",
      "- TEST VAE LOSS : 60.26287841796875\n",
      "성능 및 손실 개선이 없어서 209 EPOCH에 학습 중단\n",
      "[210 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681222107831\n",
      "\n",
      "- TEST VAE LOSS : 60.26335525512695\n",
      "성능 및 손실 개선이 없어서 210 EPOCH에 학습 중단\n",
      "[211 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226811855316164\n",
      "\n",
      "- TEST VAE LOSS : 60.26314926147461\n",
      "성능 및 손실 개선이 없어서 211 EPOCH에 학습 중단\n",
      "[212 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812761194566\n",
      "\n",
      "- TEST VAE LOSS : 60.263221740722656\n",
      "성능 및 손실 개선이 없어서 212 EPOCH에 학습 중단\n",
      "[213 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812195273006\n",
      "\n",
      "- TEST VAE LOSS : 60.2632942199707\n",
      "성능 및 손실 개선이 없어서 213 EPOCH에 학습 중단\n",
      "[214 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681248990227\n",
      "\n",
      "- TEST VAE LOSS : 60.26336669921875\n",
      "성능 및 손실 개선이 없어서 214 EPOCH에 학습 중단\n",
      "[215 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681278183881\n",
      "\n",
      "- TEST VAE LOSS : 60.26347351074219\n",
      "성능 및 손실 개선이 없어서 215 EPOCH에 학습 중단\n",
      "[216 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226811352898096\n",
      "\n",
      "- TEST VAE LOSS : 60.26301574707031\n",
      "성능 및 손실 개선이 없어서 216 EPOCH에 학습 중단\n",
      "[217 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681118258308\n",
      "\n",
      "- TEST VAE LOSS : 60.26327133178711\n",
      "성능 및 손실 개선이 없어서 217 EPOCH에 학습 중단\n",
      "[218 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681285813275\n",
      "\n",
      "- TEST VAE LOSS : 60.26312255859375\n",
      "성능 및 손실 개선이 없어서 218 EPOCH에 학습 중단\n",
      "[219 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812821332146\n",
      "\n",
      "- TEST VAE LOSS : 60.263328552246094\n",
      "성능 및 손실 개선이 없어서 219 EPOCH에 학습 중단\n",
      "[220 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812280991496\n",
      "\n",
      "- TEST VAE LOSS : 60.2633056640625\n",
      "성능 및 손실 개선이 없어서 220 EPOCH에 학습 중단\n",
      "[221 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681292096306\n",
      "\n",
      "- TEST VAE LOSS : 60.263099670410156\n",
      "성능 및 손실 개선이 없어서 221 EPOCH에 학습 중단\n",
      "[222 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226813116634595\n",
      "\n",
      "- TEST VAE LOSS : 60.26311492919922\n",
      "성능 및 손실 개선이 없어서 222 EPOCH에 학습 중단\n",
      "[223 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2268123626709\n",
      "\n",
      "- TEST VAE LOSS : 60.26325607299805\n",
      "성능 및 손실 개선이 없어서 223 EPOCH에 학습 중단\n",
      "[224 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681277241426\n",
      "\n",
      "- TEST VAE LOSS : 60.26309585571289\n",
      "성능 및 손실 개선이 없어서 224 EPOCH에 학습 중단\n",
      "[225 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2268130089255\n",
      "\n",
      "- TEST VAE LOSS : 60.26288986206055\n",
      "성능 및 손실 개선이 없어서 225 EPOCH에 학습 중단\n",
      "[226 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681161813175\n",
      "\n",
      "- TEST VAE LOSS : 60.263240814208984\n",
      "성능 및 손실 개선이 없어서 226 EPOCH에 학습 중단\n",
      "[227 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681262162153\n",
      "\n",
      "- TEST VAE LOSS : 60.263458251953125\n",
      "성능 및 손실 개선이 없어서 227 EPOCH에 학습 중단\n",
      "[228 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681258302576\n",
      "\n",
      "- TEST VAE LOSS : 60.263343811035156\n",
      "성능 및 손실 개선이 없어서 228 EPOCH에 학습 중단\n",
      "[229 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681280248305\n",
      "\n",
      "- TEST VAE LOSS : 60.262943267822266\n",
      "성능 및 손실 개선이 없어서 229 EPOCH에 학습 중단\n",
      "[230 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681282941033\n",
      "\n",
      "- TEST VAE LOSS : 60.26342010498047\n",
      "성능 및 손실 개선이 없어서 230 EPOCH에 학습 중단\n",
      "[231 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812221751494\n",
      "\n",
      "- TEST VAE LOSS : 60.26347732543945\n",
      "성능 및 손실 개선이 없어서 231 EPOCH에 학습 중단\n",
      "[232 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681254846909\n",
      "\n",
      "- TEST VAE LOSS : 60.26335144042969\n",
      "성능 및 손실 개선이 없어서 232 EPOCH에 학습 중단\n",
      "[233 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681275177002\n",
      "\n",
      "- TEST VAE LOSS : 60.26297378540039\n",
      "성능 및 손실 개선이 없어서 233 EPOCH에 학습 중단\n",
      "[234 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2268128601523\n",
      "\n",
      "- TEST VAE LOSS : 60.26328659057617\n",
      "성능 및 손실 개선이 없어서 234 EPOCH에 학습 중단\n",
      "[235 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681222691256\n",
      "\n",
      "- TEST VAE LOSS : 60.26307678222656\n",
      "성능 및 손실 개선이 없어서 235 EPOCH에 학습 중단\n",
      "[236 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812553405765\n",
      "\n",
      "- TEST VAE LOSS : 60.263187408447266\n",
      "성능 및 손실 개선이 없어서 236 EPOCH에 학습 중단\n",
      "[237 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681276613123\n",
      "\n",
      "- TEST VAE LOSS : 60.26298522949219\n",
      "성능 및 손실 개선이 없어서 237 EPOCH에 학습 중단\n",
      "[238 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2268129048067\n",
      "\n",
      "- TEST VAE LOSS : 60.263240814208984\n",
      "성능 및 손실 개선이 없어서 238 EPOCH에 학습 중단\n",
      "[239 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681277824851\n",
      "\n",
      "- TEST VAE LOSS : 60.2633171081543\n",
      "성능 및 손실 개선이 없어서 239 EPOCH에 학습 중단\n",
      "[240 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681319831399\n",
      "\n",
      "- TEST VAE LOSS : 60.26360321044922\n",
      "성능 및 손실 개선이 없어서 240 EPOCH에 학습 중단\n",
      "[241 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681237231984\n",
      "\n",
      "- TEST VAE LOSS : 60.263572692871094\n",
      "성능 및 손실 개선이 없어서 241 EPOCH에 학습 중단\n",
      "[242 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681367447797\n",
      "\n",
      "- TEST VAE LOSS : 60.263187408447266\n",
      "성능 및 손실 개선이 없어서 242 EPOCH에 학습 중단\n",
      "[243 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812810561235\n",
      "\n",
      "- TEST VAE LOSS : 60.263267517089844\n",
      "성능 및 손실 개선이 없어서 243 EPOCH에 학습 중단\n",
      "[244 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226811869004194\n",
      "\n",
      "- TEST VAE LOSS : 60.26360321044922\n",
      "성능 및 손실 개선이 없어서 244 EPOCH에 학습 중단\n",
      "[245 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681187304328\n",
      "\n",
      "- TEST VAE LOSS : 60.26335525512695\n",
      "성능 및 손실 개선이 없어서 245 EPOCH에 학습 중단\n",
      "[246 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681181918873\n",
      "\n",
      "- TEST VAE LOSS : 60.262664794921875\n",
      "성능 및 손실 개선이 없어서 246 EPOCH에 학습 중단\n",
      "[247 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681251458561\n",
      "\n",
      "- TEST VAE LOSS : 60.26314926147461\n",
      "성능 및 손실 개선이 없어서 247 EPOCH에 학습 중단\n",
      "[248 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681179001752\n",
      "\n",
      "- TEST VAE LOSS : 60.26311492919922\n",
      "성능 및 손실 개선이 없어서 248 EPOCH에 학습 중단\n",
      "[249 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681225226907\n",
      "\n",
      "- TEST VAE LOSS : 60.26308059692383\n",
      "성능 및 손실 개선이 없어서 249 EPOCH에 학습 중단\n",
      "[250 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226811900868135\n",
      "\n",
      "- TEST VAE LOSS : 60.26298904418945\n",
      "성능 및 손실 개선이 없어서 250 EPOCH에 학습 중단\n",
      "[251 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812791263356\n",
      "\n",
      "- TEST VAE LOSS : 60.263126373291016\n",
      "성능 및 손실 개선이 없어서 251 EPOCH에 학습 중단\n",
      "[252 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681223858104\n",
      "\n",
      "- TEST VAE LOSS : 60.26292419433594\n",
      "성능 및 손실 개선이 없어서 252 EPOCH에 학습 중단\n",
      "[253 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2268125408397\n",
      "\n",
      "- TEST VAE LOSS : 60.263187408447266\n",
      "성능 및 손실 개선이 없어서 253 EPOCH에 학습 중단\n",
      "[254 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681279754639\n",
      "\n",
      "- TEST VAE LOSS : 60.26350784301758\n",
      "성능 및 손실 개선이 없어서 254 EPOCH에 학습 중단\n",
      "[255 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681286262063\n",
      "\n",
      "- TEST VAE LOSS : 60.26340866088867\n",
      "성능 및 손실 개선이 없어서 255 EPOCH에 학습 중단\n",
      "[256 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812804278204\n",
      "\n",
      "- TEST VAE LOSS : 60.263328552246094\n",
      "성능 및 손실 개선이 없어서 256 EPOCH에 학습 중단\n",
      "[257 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681323556339\n",
      "\n",
      "- TEST VAE LOSS : 60.2628059387207\n",
      "성능 및 손실 개선이 없어서 257 EPOCH에 학습 중단\n",
      "[258 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812373890596\n",
      "\n",
      "- TEST VAE LOSS : 60.263267517089844\n",
      "성능 및 손실 개선이 없어서 258 EPOCH에 학습 중단\n",
      "[259 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681365158979\n",
      "\n",
      "- TEST VAE LOSS : 60.26348114013672\n",
      "성능 및 손실 개선이 없어서 259 EPOCH에 학습 중단\n",
      "[260 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2268128154979\n",
      "\n",
      "- TEST VAE LOSS : 60.26321029663086\n",
      "성능 및 손실 개선이 없어서 260 EPOCH에 학습 중단\n",
      "[261 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681186249677\n",
      "\n",
      "- TEST VAE LOSS : 60.263362884521484\n",
      "성능 및 손실 개선이 없어서 261 EPOCH에 학습 중단\n",
      "[262 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226811881570256\n",
      "\n",
      "- TEST VAE LOSS : 60.263126373291016\n",
      "성능 및 손실 개선이 없어서 262 EPOCH에 학습 중단\n",
      "[263 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2268117945054\n",
      "\n",
      "- TEST VAE LOSS : 60.26307678222656\n",
      "성능 및 손실 개선이 없어서 263 EPOCH에 학습 중단\n",
      "[264 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681248563879\n",
      "\n",
      "- TEST VAE LOSS : 60.26280975341797\n",
      "성능 및 손실 개선이 없어서 264 EPOCH에 학습 중단\n",
      "[265 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681176353903\n",
      "\n",
      "- TEST VAE LOSS : 60.26335525512695\n",
      "성능 및 손실 개선이 없어서 265 EPOCH에 학습 중단\n",
      "[266 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681227201574\n",
      "\n",
      "- TEST VAE LOSS : 60.263275146484375\n",
      "성능 및 손실 개선이 없어서 266 EPOCH에 학습 중단\n",
      "[267 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681185868207\n",
      "\n",
      "- TEST VAE LOSS : 60.26312255859375\n",
      "성능 및 손실 개선이 없어서 267 EPOCH에 학습 중단\n",
      "[268 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681277286305\n",
      "\n",
      "- TEST VAE LOSS : 60.26340103149414\n",
      "성능 및 손실 개선이 없어서 268 EPOCH에 학습 중단\n",
      "[269 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681222870771\n",
      "\n",
      "- TEST VAE LOSS : 60.26303482055664\n",
      "성능 및 손실 개선이 없어서 269 EPOCH에 학습 중단\n",
      "[270 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812552508186\n",
      "\n",
      "- TEST VAE LOSS : 60.263404846191406\n",
      "성능 및 손실 개선이 없어서 270 EPOCH에 학습 중단\n",
      "[271 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812792160935\n",
      "\n",
      "- TEST VAE LOSS : 60.262855529785156\n",
      "성능 및 손실 개선이 없어서 271 EPOCH에 학습 중단\n",
      "[272 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681288506003\n",
      "\n",
      "- TEST VAE LOSS : 60.26306915283203\n",
      "성능 및 손실 개선이 없어서 272 EPOCH에 학습 중단\n",
      "[273 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681176757813\n",
      "\n",
      "- TEST VAE LOSS : 60.263404846191406\n",
      "성능 및 손실 개선이 없어서 273 EPOCH에 학습 중단\n",
      "[274 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681186922859\n",
      "\n",
      "- TEST VAE LOSS : 60.26324462890625\n",
      "성능 및 손실 개선이 없어서 274 EPOCH에 학습 중단\n",
      "[275 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226811758602366\n",
      "\n",
      "- TEST VAE LOSS : 60.2630500793457\n",
      "성능 및 손실 개선이 없어서 275 EPOCH에 학습 중단\n",
      "[276 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812484292424\n",
      "\n",
      "- TEST VAE LOSS : 60.26328659057617\n",
      "성능 및 손실 개선이 없어서 276 EPOCH에 학습 중단\n",
      "[277 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681179854449\n",
      "\n",
      "- TEST VAE LOSS : 60.262969970703125\n",
      "성능 및 손실 개선이 없어서 277 EPOCH에 학습 중단\n",
      "[278 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681188919965\n",
      "\n",
      "- TEST VAE LOSS : 60.26285934448242\n",
      "성능 및 손실 개선이 없어서 278 EPOCH에 학습 중단\n",
      "[279 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226811975815714\n",
      "\n",
      "- TEST VAE LOSS : 60.263031005859375\n",
      "성능 및 손실 개선이 없어서 279 EPOCH에 학습 중단\n",
      "[280 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812375685746\n",
      "\n",
      "- TEST VAE LOSS : 60.263153076171875\n",
      "성능 및 손실 개선이 없어서 280 EPOCH에 학습 중단\n",
      "[281 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681252378576\n",
      "\n",
      "- TEST VAE LOSS : 60.263519287109375\n",
      "성능 및 손실 개선이 없어서 281 EPOCH에 학습 중단\n",
      "[282 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681267592486\n",
      "\n",
      "- TEST VAE LOSS : 60.263221740722656\n",
      "성능 및 손실 개선이 없어서 282 EPOCH에 학습 중단\n",
      "[283 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681163608327\n",
      "\n",
      "- TEST VAE LOSS : 60.26327896118164\n",
      "성능 및 손실 개선이 없어서 283 EPOCH에 학습 중단\n",
      "[284 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681263845107\n",
      "\n",
      "- TEST VAE LOSS : 60.26272964477539\n",
      "성능 및 손실 개선이 없어서 284 EPOCH에 학습 중단\n",
      "[285 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681254667394\n",
      "\n",
      "- TEST VAE LOSS : 60.263092041015625\n",
      "성능 및 손실 개선이 없어서 285 EPOCH에 학습 중단\n",
      "[286 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681278363396\n",
      "\n",
      "- TEST VAE LOSS : 60.26273727416992\n",
      "성능 및 손실 개선이 없어서 286 EPOCH에 학습 중단\n",
      "[287 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2268128527473\n",
      "\n",
      "- TEST VAE LOSS : 60.2631950378418\n",
      "성능 및 손실 개선이 없어서 287 EPOCH에 학습 중단\n",
      "[288 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681223005407\n",
      "\n",
      "- TEST VAE LOSS : 60.262943267822266\n",
      "성능 및 손실 개선이 없어서 288 EPOCH에 학습 중단\n",
      "[289 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681253769819\n",
      "\n",
      "- TEST VAE LOSS : 60.26319122314453\n",
      "성능 및 손실 개선이 없어서 289 EPOCH에 학습 중단\n",
      "[290 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681278385836\n",
      "\n",
      "- TEST VAE LOSS : 60.26265335083008\n",
      "성능 및 손실 개선이 없어서 290 EPOCH에 학습 중단\n",
      "[291 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681178261252\n",
      "\n",
      "- TEST VAE LOSS : 60.263145446777344\n",
      "성능 및 손실 개선이 없어서 291 EPOCH에 학습 중단\n",
      "[292 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681155934053\n",
      "\n",
      "- TEST VAE LOSS : 60.26301574707031\n",
      "성능 및 손실 개선이 없어서 292 EPOCH에 학습 중단\n",
      "[293 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812792160935\n",
      "\n",
      "- TEST VAE LOSS : 60.26334762573242\n",
      "성능 및 손실 개선이 없어서 293 EPOCH에 학습 중단\n",
      "[294 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681306547277\n",
      "\n",
      "- TEST VAE LOSS : 60.263248443603516\n",
      "성능 및 손실 개선이 없어서 294 EPOCH에 학습 중단\n",
      "[295 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681270734002\n",
      "\n",
      "- TEST VAE LOSS : 60.263492584228516\n",
      "성능 및 손실 개선이 없어서 295 EPOCH에 학습 중단\n",
      "[296 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681304348216\n",
      "\n",
      "- TEST VAE LOSS : 60.26325607299805\n",
      "성능 및 손실 개선이 없어서 296 EPOCH에 학습 중단\n",
      "[297 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681176264146\n",
      "\n",
      "- TEST VAE LOSS : 60.263145446777344\n",
      "성능 및 손실 개선이 없어서 297 EPOCH에 학습 중단\n",
      "[298 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681227874756\n",
      "\n",
      "- TEST VAE LOSS : 60.26312255859375\n",
      "성능 및 손실 개선이 없어서 298 EPOCH에 학습 중단\n",
      "[299 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681187304328\n",
      "\n",
      "- TEST VAE LOSS : 60.26333999633789\n",
      "성능 및 손실 개선이 없어서 299 EPOCH에 학습 중단\n",
      "[300 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681276613123\n",
      "\n",
      "- TEST VAE LOSS : 60.263336181640625\n",
      "성능 및 손실 개선이 없어서 300 EPOCH에 학습 중단\n",
      "[301 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681223027846\n",
      "\n",
      "- TEST VAE LOSS : 60.26346206665039\n",
      "성능 및 손실 개선이 없어서 301 EPOCH에 학습 중단\n",
      "[302 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2268125260297\n",
      "\n",
      "- TEST VAE LOSS : 60.26308822631836\n",
      "성능 및 손실 개선이 없어서 302 EPOCH에 학습 중단\n",
      "[303 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681274324305\n",
      "\n",
      "- TEST VAE LOSS : 60.263145446777344\n",
      "성능 및 손실 개선이 없어서 303 EPOCH에 학습 중단\n",
      "[304 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681283838609\n",
      "\n",
      "- TEST VAE LOSS : 60.263145446777344\n",
      "성능 및 손실 개선이 없어서 304 EPOCH에 학습 중단\n",
      "[305 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226812016655416\n",
      "\n",
      "- TEST VAE LOSS : 60.262821197509766\n",
      "성능 및 손실 개선이 없어서 305 EPOCH에 학습 중단\n",
      "[306 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681177924661\n",
      "\n",
      "- TEST VAE LOSS : 60.263092041015625\n",
      "성능 및 손실 개선이 없어서 306 EPOCH에 학습 중단\n",
      "[307 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22681247307273\n",
      "\n",
      "- TEST VAE LOSS : 60.26325607299805\n",
      "성능 및 손실 개선이 없어서 307 EPOCH에 학습 중단\n",
      "[308 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226751376208135\n",
      "\n",
      "- TEST VAE LOSS : 60.26261520385742\n",
      "성능 및 손실 개선이 없어서 308 EPOCH에 학습 중단\n",
      "[309 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673641743379\n",
      "\n",
      "- TEST VAE LOSS : 60.262611389160156\n",
      "성능 및 손실 개선이 없어서 309 EPOCH에 학습 중단\n",
      "[310 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736425960766\n",
      "\n",
      "- TEST VAE LOSS : 60.26287841796875\n",
      "성능 및 손실 개선이 없어서 310 EPOCH에 학습 중단\n",
      "[311 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736427082734\n",
      "\n",
      "- TEST VAE LOSS : 60.262638092041016\n",
      "성능 및 손실 개선이 없어서 311 EPOCH에 학습 중단\n",
      "[312 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736430448646\n",
      "\n",
      "- TEST VAE LOSS : 60.26283645629883\n",
      "성능 및 손실 개선이 없어서 312 EPOCH에 학습 중단\n",
      "[313 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2267364246144\n",
      "\n",
      "- TEST VAE LOSS : 60.262752532958984\n",
      "성능 및 손실 개선이 없어서 313 EPOCH에 학습 중단\n",
      "[314 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673642999985\n",
      "\n",
      "- TEST VAE LOSS : 60.262691497802734\n",
      "성능 및 손실 개선이 없어서 314 EPOCH에 학습 중단\n",
      "[315 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673643314137\n",
      "\n",
      "- TEST VAE LOSS : 60.26255798339844\n",
      "성능 및 손실 개선이 없어서 315 EPOCH에 학습 중단\n",
      "[316 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673643852683\n",
      "\n",
      "- TEST VAE LOSS : 60.262935638427734\n",
      "성능 및 손실 개선이 없어서 316 EPOCH에 학습 중단\n",
      "[317 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736443463494\n",
      "\n",
      "- TEST VAE LOSS : 60.26273727416992\n",
      "성능 및 손실 개선이 없어서 317 EPOCH에 학습 중단\n",
      "[318 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673645109289\n",
      "\n",
      "- TEST VAE LOSS : 60.262977600097656\n",
      "성능 및 손실 개선이 없어서 318 EPOCH에 학습 중단\n",
      "[319 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2267364506441\n",
      "\n",
      "- TEST VAE LOSS : 60.262420654296875\n",
      "성능 및 손실 개선이 없어서 319 EPOCH에 학습 중단\n",
      "[320 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736447053796\n",
      "\n",
      "- TEST VAE LOSS : 60.26229476928711\n",
      "성능 및 손실 개선이 없어서 320 EPOCH에 학습 중단\n",
      "[321 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736454234405\n",
      "\n",
      "- TEST VAE LOSS : 60.26274108886719\n",
      "성능 및 손실 개선이 없어서 321 EPOCH에 학습 중단\n",
      "[322 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673645221486\n",
      "\n",
      "- TEST VAE LOSS : 60.2628173828125\n",
      "성능 및 손실 개선이 없어서 322 EPOCH에 학습 중단\n",
      "[323 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673645288804\n",
      "\n",
      "- TEST VAE LOSS : 60.26274490356445\n",
      "성능 및 손실 개선이 없어서 323 EPOCH에 학습 중단\n",
      "[324 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736456029556\n",
      "\n",
      "- TEST VAE LOSS : 60.262603759765625\n",
      "성능 및 손실 개선이 없어서 324 EPOCH에 학습 중단\n",
      "[325 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736458722286\n",
      "\n",
      "- TEST VAE LOSS : 60.26252746582031\n",
      "성능 및 손실 개선이 없어서 325 EPOCH에 학습 중단\n",
      "[326 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736469044404\n",
      "\n",
      "- TEST VAE LOSS : 60.26298522949219\n",
      "성능 및 손실 개선이 없어서 326 EPOCH에 학습 중단\n",
      "[327 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673646949319\n",
      "\n",
      "- TEST VAE LOSS : 60.26249313354492\n",
      "성능 및 손실 개선이 없어서 327 EPOCH에 학습 중단\n",
      "[328 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736476898196\n",
      "\n",
      "- TEST VAE LOSS : 60.262718200683594\n",
      "성능 및 손실 개선이 없어서 328 EPOCH에 학습 중단\n",
      "[329 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673648654713\n",
      "\n",
      "- TEST VAE LOSS : 60.262691497802734\n",
      "성능 및 손실 개선이 없어서 329 EPOCH에 학습 중단\n",
      "[330 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736477122586\n",
      "\n",
      "- TEST VAE LOSS : 60.26255416870117\n",
      "성능 및 손실 개선이 없어서 330 EPOCH에 학습 중단\n",
      "[331 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673648138607\n",
      "\n",
      "- TEST VAE LOSS : 60.2625617980957\n",
      "성능 및 손실 개선이 없어서 331 EPOCH에 학습 중단\n",
      "[332 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736485425164\n",
      "\n",
      "- TEST VAE LOSS : 60.26241683959961\n",
      "성능 및 손실 개선이 없어서 332 EPOCH에 학습 중단\n",
      "[333 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673649731804\n",
      "\n",
      "- TEST VAE LOSS : 60.26310348510742\n",
      "성능 및 손실 개선이 없어서 333 EPOCH에 학습 중단\n",
      "[334 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673650898653\n",
      "\n",
      "- TEST VAE LOSS : 60.262638092041016\n",
      "성능 및 손실 개선이 없어서 334 EPOCH에 학습 중단\n",
      "[335 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673651930865\n",
      "\n",
      "- TEST VAE LOSS : 60.26249694824219\n",
      "성능 및 손실 개선이 없어서 335 EPOCH에 학습 중단\n",
      "[336 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673656688017\n",
      "\n",
      "- TEST VAE LOSS : 60.26296615600586\n",
      "성능 및 손실 개선이 없어서 336 EPOCH에 학습 중단\n",
      "[337 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736470839555\n",
      "\n",
      "- TEST VAE LOSS : 60.2629280090332\n",
      "성능 및 손실 개선이 없어서 337 EPOCH에 학습 중단\n",
      "[338 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673642730713\n",
      "\n",
      "- TEST VAE LOSS : 60.26246643066406\n",
      "성능 및 손실 개선이 없어서 338 EPOCH에 학습 중단\n",
      "[339 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673651212804\n",
      "\n",
      "- TEST VAE LOSS : 60.26279067993164\n",
      "성능 및 손실 개선이 없어서 339 EPOCH에 학습 중단\n",
      "[340 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673653434305\n",
      "\n",
      "- TEST VAE LOSS : 60.262508392333984\n",
      "성능 및 손실 개선이 없어서 340 EPOCH에 학습 중단\n",
      "[341 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673655947517\n",
      "\n",
      "- TEST VAE LOSS : 60.262760162353516\n",
      "성능 및 손실 개선이 없어서 341 EPOCH에 학습 중단\n",
      "[342 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673644413668\n",
      "\n",
      "- TEST VAE LOSS : 60.262840270996094\n",
      "성능 및 손실 개선이 없어서 342 EPOCH에 학습 중단\n",
      "[343 / 100000]\n",
      "- TRAIN VAE LOSS : 60.2267364248388\n",
      "\n",
      "- TEST VAE LOSS : 60.26301574707031\n",
      "성능 및 손실 개선이 없어서 343 EPOCH에 학습 중단\n",
      "[344 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673642730713\n",
      "\n",
      "- TEST VAE LOSS : 60.26276397705078\n",
      "성능 및 손실 개선이 없어서 344 EPOCH에 학습 중단\n",
      "[345 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673651212804\n",
      "\n",
      "- TEST VAE LOSS : 60.26264953613281\n",
      "성능 및 손실 개선이 없어서 345 EPOCH에 학습 중단\n",
      "[346 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736533669865\n",
      "\n",
      "- TEST VAE LOSS : 60.26283645629883\n",
      "성능 및 손실 개선이 없어서 346 EPOCH에 학습 중단\n",
      "[347 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673655969956\n",
      "\n",
      "- TEST VAE LOSS : 60.26285171508789\n",
      "성능 및 손실 개선이 없어서 347 EPOCH에 학습 중단\n",
      "[348 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673644413668\n",
      "\n",
      "- TEST VAE LOSS : 60.262901306152344\n",
      "성능 및 손실 개선이 없어서 348 EPOCH에 학습 중단\n",
      "[349 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673642506319\n",
      "\n",
      "- TEST VAE LOSS : 60.26283264160156\n",
      "성능 및 손실 개선이 없어서 349 EPOCH에 학습 중단\n",
      "[350 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673642730713\n",
      "\n",
      "- TEST VAE LOSS : 60.26286697387695\n",
      "성능 및 손실 개선이 없어서 350 EPOCH에 학습 중단\n",
      "[351 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673651212804\n",
      "\n",
      "- TEST VAE LOSS : 60.2625617980957\n",
      "성능 및 손실 개선이 없어서 351 EPOCH에 학습 중단\n",
      "[352 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673653344547\n",
      "\n",
      "- TEST VAE LOSS : 60.26304244995117\n",
      "성능 및 손실 개선이 없어서 352 EPOCH에 학습 중단\n",
      "[353 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673655969956\n",
      "\n",
      "- TEST VAE LOSS : 60.26288604736328\n",
      "성능 및 손실 개선이 없어서 353 EPOCH에 학습 중단\n",
      "[354 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673644436107\n",
      "\n",
      "- TEST VAE LOSS : 60.262760162353516\n",
      "성능 및 손실 개선이 없어서 354 EPOCH에 학습 중단\n",
      "[355 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673642506319\n",
      "\n",
      "- TEST VAE LOSS : 60.26274108886719\n",
      "성능 및 손실 개선이 없어서 355 EPOCH에 학습 중단\n",
      "[356 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736427082734\n",
      "\n",
      "- TEST VAE LOSS : 60.262962341308594\n",
      "성능 및 손실 개선이 없어서 356 EPOCH에 학습 중단\n",
      "[357 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673651212804\n",
      "\n",
      "- TEST VAE LOSS : 60.2623176574707\n",
      "성능 및 손실 개선이 없어서 357 EPOCH에 학습 중단\n",
      "[358 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736533669865\n",
      "\n",
      "- TEST VAE LOSS : 60.2625846862793\n",
      "성능 및 손실 개선이 없어서 358 EPOCH에 학습 중단\n",
      "[359 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673655969956\n",
      "\n",
      "- TEST VAE LOSS : 60.26247787475586\n",
      "성능 및 손실 개선이 없어서 359 EPOCH에 학습 중단\n",
      "[360 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736445258645\n",
      "\n",
      "- TEST VAE LOSS : 60.26299285888672\n",
      "성능 및 손실 개선이 없어서 360 EPOCH에 학습 중단\n",
      "[361 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673642506319\n",
      "\n",
      "- TEST VAE LOSS : 60.2624397277832\n",
      "성능 및 손실 개선이 없어서 361 EPOCH에 학습 중단\n",
      "[362 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673642730713\n",
      "\n",
      "- TEST VAE LOSS : 60.26244354248047\n",
      "성능 및 손실 개선이 없어서 362 EPOCH에 학습 중단\n",
      "[363 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673651212804\n",
      "\n",
      "- TEST VAE LOSS : 60.26277160644531\n",
      "성능 및 손실 개선이 없어서 363 EPOCH에 학습 중단\n",
      "[364 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736533669865\n",
      "\n",
      "- TEST VAE LOSS : 60.263023376464844\n",
      "성능 및 손실 개선이 없어서 364 EPOCH에 학습 중단\n",
      "[365 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673655969956\n",
      "\n",
      "- TEST VAE LOSS : 60.262725830078125\n",
      "성능 및 손실 개선이 없어서 365 EPOCH에 학습 중단\n",
      "[366 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673644436107\n",
      "\n",
      "- TEST VAE LOSS : 60.26287078857422\n",
      "성능 및 손실 개선이 없어서 366 EPOCH에 학습 중단\n",
      "[367 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673642506319\n",
      "\n",
      "- TEST VAE LOSS : 60.26276397705078\n",
      "성능 및 손실 개선이 없어서 367 EPOCH에 학습 중단\n",
      "[368 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673642730713\n",
      "\n",
      "- TEST VAE LOSS : 60.262794494628906\n",
      "성능 및 손실 개선이 없어서 368 EPOCH에 학습 중단\n",
      "[369 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673651212804\n",
      "\n",
      "- TEST VAE LOSS : 60.26259994506836\n",
      "성능 및 손실 개선이 없어서 369 EPOCH에 학습 중단\n",
      "[370 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736533669865\n",
      "\n",
      "- TEST VAE LOSS : 60.26277542114258\n",
      "성능 및 손실 개선이 없어서 370 EPOCH에 학습 중단\n",
      "[371 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673655969956\n",
      "\n",
      "- TEST VAE LOSS : 60.262969970703125\n",
      "성능 및 손실 개선이 없어서 371 EPOCH에 학습 중단\n",
      "[372 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673644436107\n",
      "\n",
      "- TEST VAE LOSS : 60.262813568115234\n",
      "성능 및 손실 개선이 없어서 372 EPOCH에 학습 중단\n",
      "[373 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673642506319\n",
      "\n",
      "- TEST VAE LOSS : 60.26265335083008\n",
      "성능 및 손실 개선이 없어서 373 EPOCH에 학습 중단\n",
      "[374 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736427082734\n",
      "\n",
      "- TEST VAE LOSS : 60.26263427734375\n",
      "성능 및 손실 개선이 없어서 374 EPOCH에 학습 중단\n",
      "[375 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673651302562\n",
      "\n",
      "- TEST VAE LOSS : 60.262630462646484\n",
      "성능 및 손실 개선이 없어서 375 EPOCH에 학습 중단\n",
      "[376 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736533669865\n",
      "\n",
      "- TEST VAE LOSS : 60.26283264160156\n",
      "성능 및 손실 개선이 없어서 376 EPOCH에 학습 중단\n",
      "[377 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673655947517\n",
      "\n",
      "- TEST VAE LOSS : 60.2626838684082\n",
      "성능 및 손실 개선이 없어서 377 EPOCH에 학습 중단\n",
      "[378 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673644413668\n",
      "\n",
      "- TEST VAE LOSS : 60.26250076293945\n",
      "성능 및 손실 개선이 없어서 378 EPOCH에 학습 중단\n",
      "[379 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673642506319\n",
      "\n",
      "- TEST VAE LOSS : 60.262630462646484\n",
      "성능 및 손실 개선이 없어서 379 EPOCH에 학습 중단\n",
      "[380 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673642730713\n",
      "\n",
      "- TEST VAE LOSS : 60.26310348510742\n",
      "성능 및 손실 개선이 없어서 380 EPOCH에 학습 중단\n",
      "[381 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673651212804\n",
      "\n",
      "- TEST VAE LOSS : 60.26277160644531\n",
      "성능 및 손실 개선이 없어서 381 EPOCH에 학습 중단\n",
      "[382 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736533669865\n",
      "\n",
      "- TEST VAE LOSS : 60.262874603271484\n",
      "성능 및 손실 개선이 없어서 382 EPOCH에 학습 중단\n",
      "[383 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673655969956\n",
      "\n",
      "- TEST VAE LOSS : 60.26272201538086\n",
      "성능 및 손실 개선이 없어서 383 EPOCH에 학습 중단\n",
      "[384 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673644436107\n",
      "\n",
      "- TEST VAE LOSS : 60.26258087158203\n",
      "성능 및 손실 개선이 없어서 384 EPOCH에 학습 중단\n",
      "[385 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673642506319\n",
      "\n",
      "- TEST VAE LOSS : 60.26296615600586\n",
      "성능 및 손실 개선이 없어서 385 EPOCH에 학습 중단\n",
      "[386 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673642730713\n",
      "\n",
      "- TEST VAE LOSS : 60.262264251708984\n",
      "성능 및 손실 개선이 없어서 386 EPOCH에 학습 중단\n",
      "[387 / 100000]\n",
      "- TRAIN VAE LOSS : 60.22673651212804\n",
      "\n",
      "- TEST VAE LOSS : 60.262733459472656\n",
      "성능 및 손실 개선이 없어서 387 EPOCH에 학습 중단\n",
      "[388 / 100000]\n",
      "- TRAIN VAE LOSS : 60.226736533669865\n",
      "\n",
      "- TEST VAE LOSS : 60.262939453125\n",
      "성능 및 손실 개선이 없어서 388 EPOCH에 학습 중단\n"
     ]
    }
   ],
   "source": [
    "vae_loss = training(water_X_test, water_y_test, vae_model, water_trainDL,\n",
    "                    optimizer, EPOCH, scheduler, DEVICE, accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
