{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자 정의 모델 클래스\n",
    "- 부모클래스 : nn.Module\n",
    "- 필수 오버라이딩\n",
    "    * _ _ init _ _ () : 모델 층 구성 즉, 설계\n",
    "    * forward() : 순방향 학습 진행 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈로딩\n",
    "import torch                               # 텐서 관련 모듈\n",
    "import torch.nn as nn                      # 인공신경망 관련 모듈\n",
    "import torch.nn.functional as F            # 인공신경망 관련 함수들 모듈 ( 손실함수, 활성화 함수 등등)\n",
    "import torch.optim as optim                # 최적화 관련 모듈 (가중치, 절편 빠르게 찾아주는 알고리즘)\n",
    "from torchinfo import summary              # 모델 구조 및 정보 관련 모듈\n",
    "from torchmetrics.regression import *      # 회귀 성능 지표 관련 모듈\n",
    "from torchmetrics.classification import *  # 분류 성능 지표 관련 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 고정\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 텐서 저장 및 실행 위치\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [기본] 신경망 클래스 <hr>\n",
    "    * 입력층 - 입력 : 피쳐수 고정\n",
    "    * 출력층 - 출력 : 타겟수 고정\n",
    "    * 은닉층 - 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "# 데이터셋 : 피쳐 4개, 타겟 1개, 회귀\n",
    "# 입 력 층 : 입력    4개    출력  20개    AF  ReLU\n",
    "# 은 닉 층 : 입력   20개    출력  100개   AF  ReLU\n",
    "# 출 력 층 : 입력  100개    출력  1개     AF  X , (Sigmoid & Softmax) 분류 일 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    # 인스턴스 / 객체 생성 시 자동 호출되는 메서드 (콜백함수 Callback func)\n",
    "    def __init__(self):\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(3, 20)     # W 4 + b 1 => 1P, 5 * 20 = 100개 변수\n",
    "        self.hidden_layer = nn.Linear(20, 100)  # W 20 + b 1 => 1P, 21 * 100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(100, 1)      # W 100 + b 1 => 1P, 101 * 1 = 101개 변수\n",
    "\n",
    "\n",
    "    # 순방향 / 전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)     # 1개 퍼셉트론 : y = x1W1 + x2W2 + x3W3 + x4W4 + b\n",
    "        y = F.relu(y)               # 0 <= y    ---->   죽은 relu ==> leakyReLu\n",
    "        \n",
    "        y = self.hidden_layer(y)    # 1개 퍼셉트론 : y = x1W1 + x2W2 + ~ + x20W20 + b\n",
    "        y = F.relu(y)               \n",
    "\n",
    "        return self.output_layer(y)     # 1개 퍼셉트론 : y = x1W1 + x2W2 + ~ + x100W100 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수가 동적인 모델\n",
    "class MyModel2(nn.Module):\n",
    "    \n",
    "    # 인스턴스 / 객체 생성 시 자동 호출되는 메서드 (콜백함수 Callback func)\n",
    "    def __init__(self, in_features):\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_features, 20)     # W 4 + b 1 => 1P, 5 * 20 = 100개 변수\n",
    "        self.hidden_layer = nn.Linear(20, 100)  # W 20 + b 1 => 1P, 21 * 100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(100, 1)      # W 100 + b 1 => 1P, 101 * 1 = 101개 변수\n",
    "\n",
    "\n",
    "    # 순방향 / 전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)     # 1개 퍼셉트론 : y = x1W1 + x2W2 + x3W3 + x4W4 + b\n",
    "        y = F.relu(y)               # 0 <= y    ---->   죽은 relu ==> leakyReLu\n",
    "        \n",
    "        y = self.hidden_layer(y)    # 1개 퍼셉트론 : y = x1W1 + x2W2 + ~ + x20W20 + b\n",
    "        y = F.relu(y)               \n",
    "\n",
    "        return self.output_layer(y)     # 1개 퍼셉트론 : y = x1W1 + x2W2 + ~ + x100W100 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수가 동적인 모델델\n",
    "class MyModel3(nn.Module):\n",
    "    \n",
    "    # 인스턴스 / 객체 생성 시 자동 호출되는 메서드 (콜백함수 Callback func)\n",
    "    def __init__(self, in_features, in_out, h_out):\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_features, in_out)     # W 4 + b 1 => 1P, 5 * 20 = 100개 변수\n",
    "        self.hidden_layer = nn.Linear(in_out, h_out)  # W 20 + b 1 => 1P, 21 * 100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(h_out, 1)      # W 100 + b 1 => 1P, 101 * 1 = 101개 변수\n",
    "\n",
    "\n",
    "    # 순방향 / 전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)     # 1개 퍼셉트론 : y = x1W1 + x2W2 + x3W3 + x4W4 + b\n",
    "        y = F.relu(y)               # 0 <= y    ---->   죽은 relu ==> leakyReLu\n",
    "        \n",
    "        y = self.hidden_layer(y)    # 1개 퍼셉트론 : y = x1W1 + x2W2 + ~ + x20W20 + b\n",
    "        y = F.relu(y)               \n",
    "\n",
    "        return self.output_layer(y)     # 1개 퍼셉트론 : y = x1W1 + x2W2 + ~ + x100W100 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 인스턴스 생성\n",
    "m1 = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input_layer.weight', Parameter containing:\n",
      "tensor([[ 0.2975, -0.2548, -0.1119],\n",
      "        [ 0.2710, -0.5435,  0.3462],\n",
      "        [-0.1188,  0.2937,  0.0803],\n",
      "        [-0.0707,  0.1601,  0.0285],\n",
      "        [ 0.2109, -0.2250, -0.0421],\n",
      "        [-0.0520,  0.0837, -0.0023],\n",
      "        [ 0.5047,  0.1797, -0.2150],\n",
      "        [-0.3487, -0.0968, -0.2490],\n",
      "        [-0.1850,  0.0276,  0.3442],\n",
      "        [ 0.3138, -0.5644,  0.3579],\n",
      "        [ 0.1613,  0.5476,  0.3811],\n",
      "        [-0.5260, -0.5489, -0.2785],\n",
      "        [ 0.5070, -0.0962,  0.2471],\n",
      "        [-0.2683,  0.5665, -0.2443],\n",
      "        [ 0.4330,  0.0068, -0.3042],\n",
      "        [ 0.2968, -0.3065,  0.1698],\n",
      "        [-0.1667, -0.0633, -0.5551],\n",
      "        [-0.2753,  0.3133, -0.1403],\n",
      "        [ 0.5751,  0.4628, -0.0270],\n",
      "        [-0.3854,  0.3516,  0.1792]], requires_grad=True))\n",
      "('input_layer.bias', Parameter containing:\n",
      "tensor([-0.3732,  0.3750,  0.3505,  0.5120, -0.3236, -0.0950, -0.0112,  0.0843,\n",
      "        -0.4382, -0.4097,  0.3141, -0.1354,  0.2820,  0.0329,  0.1896,  0.1270,\n",
      "         0.2099,  0.2862, -0.5347,  0.2906], requires_grad=True))\n",
      "('hidden_layer.weight', Parameter containing:\n",
      "tensor([[-0.1572, -0.1687,  0.0136,  ...,  0.0401, -0.0951, -0.0677],\n",
      "        [ 0.2048, -0.0414,  0.1261,  ...,  0.1389, -0.1618, -0.1610],\n",
      "        [-0.1352,  0.0281,  0.2229,  ...,  0.0578, -0.1529, -0.1878],\n",
      "        ...,\n",
      "        [ 0.0490, -0.0862, -0.1338,  ...,  0.1569,  0.1833, -0.1882],\n",
      "        [-0.1844, -0.1471, -0.1923,  ...,  0.1118,  0.0691,  0.0738],\n",
      "        [-0.2086, -0.0753, -0.1438,  ...,  0.1593, -0.0429,  0.0194]],\n",
      "       requires_grad=True))\n",
      "('hidden_layer.bias', Parameter containing:\n",
      "tensor([-0.1867,  0.1260,  0.0129,  0.1656, -0.0943,  0.2160,  0.1491, -0.0626,\n",
      "         0.0610, -0.1631, -0.1489, -0.1532, -0.1285,  0.0622, -0.0964, -0.0772,\n",
      "         0.1564,  0.0369, -0.1090, -0.0607, -0.0588,  0.1044,  0.1574,  0.1782,\n",
      "        -0.1521, -0.0239,  0.0525, -0.0874,  0.1470, -0.0249,  0.0239, -0.1912,\n",
      "         0.1616, -0.1482, -0.0054, -0.1824,  0.1701,  0.0129,  0.1007, -0.1758,\n",
      "         0.0190,  0.0916,  0.1579,  0.1643, -0.0399, -0.0227,  0.0743, -0.0804,\n",
      "        -0.0910, -0.2059, -0.2193, -0.1177,  0.0279,  0.0180,  0.0179,  0.0335,\n",
      "        -0.0413, -0.2209,  0.1997, -0.1636, -0.0026,  0.0888,  0.0573, -0.0971,\n",
      "        -0.1943,  0.1790,  0.0064,  0.2039,  0.1043,  0.0955,  0.0484, -0.0504,\n",
      "        -0.1311,  0.0272,  0.2149,  0.1341,  0.2031,  0.2097,  0.1747, -0.1255,\n",
      "        -0.1218, -0.0077, -0.1156, -0.0607, -0.0766,  0.1957, -0.0150,  0.2164,\n",
      "        -0.1734, -0.2206,  0.0320,  0.0446, -0.0902, -0.1132,  0.0637,  0.0112,\n",
      "        -0.0300,  0.1135, -0.0951,  0.1759], requires_grad=True))\n",
      "('output_layer.weight', Parameter containing:\n",
      "tensor([[-0.0682, -0.0520,  0.0346,  0.0615, -0.0012, -0.0785, -0.0040, -0.0338,\n",
      "         -0.0320, -0.0185,  0.0241, -0.0233, -0.0701, -0.0499, -0.0838, -0.0999,\n",
      "          0.0483,  0.0559,  0.0756,  0.0079, -0.0411, -0.0601,  0.0498, -0.0604,\n",
      "          0.0747, -0.0542, -0.0933, -0.0720,  0.0814,  0.0755,  0.0244,  0.0023,\n",
      "          0.0857,  0.0603,  0.0592, -0.0681,  0.0160,  0.0906, -0.0704,  0.0996,\n",
      "         -0.0655, -0.0294, -0.0047,  0.0260,  0.0307, -0.0572,  0.0458,  0.0265,\n",
      "         -0.0111, -0.0995,  0.0990, -0.0370, -0.0533,  0.0990, -0.0507,  0.0668,\n",
      "          0.0610, -0.0651, -0.0806, -0.0544,  0.0313,  0.0943, -0.0056, -0.0299,\n",
      "         -0.0288, -0.0696,  0.0999, -0.0011, -0.0791, -0.0683,  0.0622,  0.0484,\n",
      "          0.0645, -0.0105, -0.0198,  0.0328, -0.0010, -0.0322,  0.0238, -0.0570,\n",
      "         -0.0692, -0.0221, -0.0713,  0.0025,  0.0157,  0.0979,  0.0288, -0.0006,\n",
      "         -0.0727, -0.0402, -0.0163, -0.0720,  0.0195,  0.0720,  0.0572, -0.0415,\n",
      "         -0.0244,  0.0997, -0.0160, -0.0898]], requires_grad=True))\n",
      "('output_layer.bias', Parameter containing:\n",
      "tensor([-0.0009], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for m in m1.named_parameters():\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling forward()\n",
      "tensor([[-0.1983],\n",
      "        [-0.2537]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행 ==> 모델 인스턴스 명 (데이터)\n",
    "# 임의의 데이터\n",
    "dataTS = torch.FloatTensor([[1, 3, 5], [2, 4, 6]])\n",
    "targetTS = torch.FloatTensor([[4], [5]])\n",
    "\n",
    "# 학습\n",
    "pre_y = m1(dataTS)\n",
    "\n",
    "print(pre_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling forward()\n",
      "tensor([[0.1149],\n",
      "        [0.1381]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m3 = MyModel3(3, 100, 20)\n",
    "pre_y = m3(dataTS)\n",
    "\n",
    "print(pre_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수가 동적인 모델델\n",
    "class MyModel4(nn.Module):\n",
    "    \n",
    "    # 인스턴스 / 객체 생성 시 자동 호출되는 메서드 (콜백함수 Callback func)\n",
    "    def __init__(self, in_features, in_out, h_out, h_cnt):\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_features, in_out)     # W 4 + b 1 => 1P, 5 * 20 = 100개 변수\n",
    "        \n",
    "        if h_cnt == 0:\n",
    "            pass\n",
    "\n",
    "        elif h_cnt == 1:\n",
    "            self.hidden_layer = nn.Linear(in_out, h_out)  # W 20 + b 1 => 1P, 21 * 100 = 2100개 변수\n",
    "        \n",
    "        else:\n",
    "            self.hidden_layer = nn.ModuleList()\n",
    "            self.hidden_layer.append(nn.Linear(in_out, h_out))\n",
    "\n",
    "        \n",
    "            for i in range(h_cnt - 1):\n",
    "                h_out2 = h_out + 20\n",
    "                self.hidden_layer.append(nn.Linear(h_out, h_out2))\n",
    "                h_out = h_out2\n",
    "\n",
    "        self.output_layer = nn.Linear(h_out2, 1)      # W 100 + b 1 => 1P, 101 * 1 = 101개 변수\n",
    "\n",
    "\n",
    "    # 순방향 / 전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)     # 1개 퍼셉트론 : y = x1W1 + x2W2 + x3W3 + x4W4 + b\n",
    "        y = F.relu(y)               # 0 <= y    ---->   죽은 relu ==> leakyReLu\n",
    "        \n",
    "        y = self.hidden_layer(y)    # 1개 퍼셉트론 : y = x1W1 + x2W2 + ~ + x20W20 + b\n",
    "        y = F.relu(y)               \n",
    "\n",
    "        return self.output_layer(y)     # 1개 퍼셉트론 : y = x1W1 + x2W2 + ~ + x100W100 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling forward()\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Module [ModuleList] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[275], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m m4 \u001b[38;5;241m=\u001b[39m MyModel4(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m pre_y \u001b[38;5;241m=\u001b[39m \u001b[43mm4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataTS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(pre_y)\n",
      "File \u001b[1;32mc:\\Users\\KDP-2\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KDP-2\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[274], line 37\u001b[0m, in \u001b[0;36mMyModel4.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer(x)     \u001b[38;5;66;03m# 1개 퍼셉트론 : y = x1W1 + x2W2 + x3W3 + x4W4 + b\u001b[39;00m\n\u001b[0;32m     35\u001b[0m y \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(y)               \u001b[38;5;66;03m# 0 <= y    ---->   죽은 relu ==> leakyReLu\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# 1개 퍼셉트론 : y = x1W1 + x2W2 + ~ + x20W20 + b\u001b[39;00m\n\u001b[0;32m     38\u001b[0m y \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(y)               \n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(y)\n",
      "File \u001b[1;32mc:\\Users\\KDP-2\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KDP-2\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KDP-2\\anaconda3\\envs\\TORCH_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:352\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] is missing the required \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m function\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Module [ModuleList] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "m4 = MyModel4(3, 20, 100, 5)\n",
    "\n",
    "pre_y = m4(dataTS)\n",
    "print(pre_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
